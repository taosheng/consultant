(1) install single node hadoop
  * get source: adoop-2.2.0.tar.gz, extract it, 
  * check JAVA_HOME
    if no, setup java home 
    doug@ts:~/hadoop/etc/hadoop$ export JAVA_HOME=/usr/lib/jvm/default-java

  * source hadoop-env.sh 
  * source yarn-env.sh 
   
  * based on the document-> http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/SingleCluster.html

  ==== environment settings ===

 export JAVA_HOME=/usr/lib/jvm/default-java
 export HADOOP_COMMON_HOME=/home/doug/hadoop
 export HADOOP_HDFS_HOME=/home/doug/hadoop
 export HADOO_MAPRED_HOME=/home/doug/hadoop
 export HADOOP_YARN_HOME=/home/doug/hadoop
 export HADOOP_CONF_DIR=/home/doug/hadoop/etc/hadoop
 expor tYARN_CONF_DIR=$HADOOP_CONF_DIR
 export PATH=$PATH:/home/doug/hadoop/bin

 * run the example ./hadoop/bin/hadoop jar hadoop-examples.jar randomwriter out
  the hadoop-examples.jar download from ->wget http://www.java2s.com/Code/JarDownload/hadoop-examples/hadoop-examples.jar.zip

(2) prepare environment

 * in 2.2.0 the "hadoop dfs..." commend should be replaced by "hdfs dfs..."
hadoop@hd1:~$ hadoop dfs -mkdir input
Warning: $HADOOP_HOME is deprecated.

hadoop@hd1:~$ hadoop dfs -mkdir output
Warning: $HADOOP_HOME is deprecated.

hadoop@hd1:~$ hadoop dfs -ls
Warning: $HADOOP_HOME is deprecated.

Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2013-01-27 12:50 /user/hadoop/input
drwxr-xr-x   - hadoop supergroup          0 2013-01-27 12:50 /user/hadoop/output

(3) copy or git clone the 3rd party application to there...
hadoop@hd1:~/application$ ls
build  build.xml  dist  note  python  README  src  testurl.txt
hadoop@hd1:~/application$ pwd
/home/hadoop/application

(4) copy file to input /user/hadoop/input

hadoop@hd1:~/application$ hadoop dfs -copyFromLocal testurl.txt /user/hadoop/input/
hadoop@hd1:~/application$ hadoop dfs -ls /user/hadoop/input

Found 1 items
-rw-r--r--   1 hadoop supergroup        169 2013-01-27 12:58 /user/hadoop/input/testurl.txt


(5) create tmp folder :/tmp/show
hadoop@hd1:~/application$ mkdir /tmp/show

(6) execute program

* cd to hadoop home $ cd $HADOOP_HOME
* execute...(remember to change output folder name)
hadoop@hd1:/usr/local/hadoop$ hadoop jar contrib/streaming/hadoop-*streaming*.jar  -D mapred.reduce.tasks=0 -D mapred.map.tasks=2  -file /home/hadoop/application/python/linkAnalysis.py -mapper /home/hadoop/application/python/linkAnalysis.py  -input /user/hadoop/input/* -output /user/hadoop/output_newXX

* the new environment (in shared hosted server)
./bin/hadoop jar /home/doug/hadoop_source/hadoop-2.2.0/share/hadoop/tools/lib/hadoop-streaming-2.2.0.jar -D apred.reduce.tasks=0 -D mapred.map.tasks=2  -file /home/doug/github/consultant/application/python/linkAnalysis.py  -mapper /home/doug/github/consultant/application/python/linkAnalysis.py  -input /home/doug/input/* -output /home/doug/output_new01



hadoop jar /home/doug/hadoop_source/hadoop-2.2.0/share/hadoop/tools/lib/*streaming*.jar \
-file searchAnalysisMap.py  -mapper searchAnalysisMap.py \
-file searchAnalysisReducer.py -reducer searchAnalysisReducer.py \
-input test/* -output /home/doug/output_new01



